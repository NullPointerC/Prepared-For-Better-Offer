### hashmap的数据插入原理是怎么样的？

![image-20220309202946733](http://static.codenote.xyz/20220309202946.png)

如果链表长度大于8，转化为红黑树；

如果红黑树中节点个数小于6，转回链表；

以上条件都要基于数组的长度大于64时，才会进行转化；

### HashMap怎么设定初始容量大小?

一般如果new HashMap()不传值，默认大小是16，负载因子是0.75。

如果自己传入初始大小k，初始化大小为大于k的2的整数次方，例如如果传10，大小为16。

![image-20220309210013740](http://static.codenote.xyz/20220309210013.png)

在1.8中，传入的初始容量cap，真实容量会被设置为距离cap最近的2的整数次方。

在1.7中，传入的初始容量cap，真实容量会被设置为cap。

### HashMap的哈希函数设计是怎样的?

hash函数是先拿到key 的hashcode，是一个32位的int值，然后让hashcode的高16位和低16位进行异或操作。

原因：

①.尽可能降低hash碰撞；

②.低位掩码就是利用高位掩码替换低位掩码，也称为扰动函数；

1.7中是做了4次扰动，实际效果和1.8扰动一次差异不大，所以也就采用一次扰动。

### 1.8还有别的优化吗?

数据结构层面：在1.8中底层数据结构数组+链表改成了数组+链表或红黑树;

插入方式：链表的插入方式从头插法改成了尾插法，简单说就是插入时，如果数组位置上已经有元素，1.7将新元素放到数组中，原始节点作为新节点的后继节点，1.8遍历链表将元素放置到链表的最后;

原因如下：

![image-20220310134028145](http://static.codenote.xyz/20220310134028.png)

扩容：扩容的时候1.7需要对原数组中的元素进行重新hash定位在新数组的位置,1.8采用更简单的判断逻辑，位置不变或索引+旧容量大小;

插入时：1.7先判断是否需要扩容，再插入，1.8先进行插入，插入完成再判断是否需要扩容;

### 扩容的时候为什么1.8不用重新hash就可以直接定位原节点在新数据的位置呢?

这是和每次hashmap扩容都是2倍有关的，为了不用rehash。

假设原数据为5，数组大小为16，

二者进行与运算

```txt
扩容前计算位置
	0	0	0	0	0	1	0	1（5）

& 	0	0	0	0	1	1	1	1(16-1)
----------------------------------------
	0	0	0	0	0	1	0	1(5)
扩容后计算位置
	0	0	0	0	0	1	0	1(5)
&	0	0	0	1	1	1	1	1(32-1)
----------------------------------------
	0	0	0	0	0	1	0	1(5)
```

二者得到了一样的结果

```txt
	0	0	0	1	0	1	0	1（21）
&	0	0	0	0	1	1	1	1（16-1）
------------------------------------
	0	0	0	0	0	1	0	1(扩容前是5)
扩容后
	0	0	0	1	0	1	0	1
&	0	0	0	1	1	1	1	1
------------------------------------
	0	0	0	1	0	1	0	1（21=5+16）
```

所以在1.8中，只需要知道最高位即可判断新的位置了。

### HashMap是线程安全的吗?

1.7会产生多线程插入死循环或者数据覆盖；

1.8也会产生数据覆盖；

数据覆盖：

A线程判断了`index`位置为空，正好挂起；

B线程判断`index`为空，写入数据；

A线程恢复，执行赋值操作，就会把B线程的数据覆盖；

还有hashmap中的++size操作也会造成线程不安全的问题；

### 怎么解决这个线程不安全的问题?

Java中有HashTable、Collections.synchronizedMap、以及ConcurrentHashMap可以实现线程安全的Map。

HashTable是直接在操作方法上加synchronized关键字，锁住整个数组，粒度比较大;

Collections.synchronizedMap是使用Collections集合工具的内部类，通过传入Map封装出一个SynchronizedMap对象，内部定义了一个对象锁，方法内通过对象锁实现;

ConcurrentHashMap使用分段锁，降低了锁粒度，让并发度大大提高。

### 1.7与1.8的ConcurrentHashMap实现有什么不同吗?

hashtable和synchronizedMap都不是真正的并发，都是需要获得集合对象的锁来保证原子性操作，不能做到两个线程真正地并发，任何线程在执行时，总要等前一个线程执行完，在高并发时，会影响系统吞吐量。

1.7中的`ConcurrentHashMap`使用`ReentrantLock+Segment+HashEntry`实现；

![image-20220310140153528](http://static.codenote.xyz/20220310140153.png)

使用拆解锁对象的方式来提高吞吐量，将整个hashmap分成若干个段，每个段都是一个子hashmap，如果新增一个entry，并不是把整个对象都锁住，而是根据hashcode得到这个entry应该被存放到哪个段当中，然后对该段上锁，完成put操作；

默认情况下有16个段，理想情况下允许有16个线程并发访问；

所以put操作需要经过两次hash操作，第一次确认在哪个段中，第二次确定在段中位置；

分段锁的优势在于保证在操作不同段 map 的时候可以并发执行，操作同段 map 的时候，进行锁的竞争和等待。这相对于直接对整个map同步synchronized是有优势的。

1.7中的分段锁的缺点在于分成很多段时浪费内存空间(不连续,碎片化)；

生产环境中，线程放入一组key-value时竞争同一个分段锁概率较小,反而造成put更新一组key-value时等操作长时间等待；

当某个段很大时，分段锁的性能会有很大的下降；

JDK1.8之后ConcurrentHashMap取消了Segment分段锁的数据结构，取而代之的是数组+链表+红黑树的结构，使用CAS保证线程安全。

### 讲讲CAS是怎么保正线程安全的?

CAS（比较与交换，Compare and swap)是一种无锁算法。

CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B,否则什么都不做，并告诉A的值实际是多少。

当多个线程同时尝试使用CAS修改一个变量时，只有一个线程能够更新变量的值，其他线程都更新失败，其他线程不会被挂起，而是被警告更新失败，从而再次尝试；

CAS的优点：

①.非阻塞式，天生免疫死锁的问题；

②.线程之间的影响比起使用锁的影响更小，竞争锁失败会引起线程挂起，CAS更新失败不会挂起；

CAS的缺点：

①.ABA问题，假设有一个变量A，修改为B，然后又修改为了A，实际已经修改过了，但CAS可能无法感知，造成了不合理的值修改操作。

可以使用AtomicStampedReference类中的compareAndSet方法，检查当前引起是否等于预期引用，当前标志是否等于预期标志，如果全部都相同，才可以修改成功；



